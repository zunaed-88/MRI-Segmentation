{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXW9aGLroXsV"
   },
   "source": [
    "<img src=\"https://miro.medium.com/max/2652/1*eTkBMyqdg9JodNcG_O4-Kw.jpeg\" width=\"100%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pTHrazBFcAtl"
   },
   "source": [
    "<a name=\"0\"></a>\n",
    "## Packages\n",
    "\n",
    "In this assignment, we'll make use of the following packages:\n",
    "\n",
    "- `keras` is a framework for building deep learning models.\n",
    "- `keras.backend` allows us to perform math operations on tensors.\n",
    "- `nibabel` will let us extract the images and labels from the files in our dataset.\n",
    "- `numpy` is a library for mathematical and scientific operations.\n",
    "-  `pandas` is what we'll use to manipulate our data.\n",
    "\n",
    "#### Import Packages\n",
    "\n",
    "Run the next cell to import all the necessary packages, dependencies and custom util functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T01:29:11.197002Z",
     "start_time": "2020-03-08T01:28:16.130730Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "colab_type": "code",
    "id": "sJz-IbUycEhT",
    "outputId": "49ae7bfe-c506-4b1a-c6ea-7a831bedfc09"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K \n",
    "\n",
    "import tensorflow as tf\n",
    "#from tensorflow.keras import engines\n",
    "#from engines import Input, Model\n",
    "from tensorflow.keras.layers import Activation, Conv3D, MaxPooling3D, UpSampling3D, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B563bDC1hUvr"
   },
   "source": [
    "<a name=\"1\"></a>\n",
    "## Dataset\n",
    "\n",
    "<a name=\"1-1\"></a>\n",
    "### What is an MRI?\n",
    "\n",
    "Magnetic resonance imaging (MRI) is an advanced imaging technique that is used to observe a variety of diseases and parts of the body. \n",
    "\n",
    "As we will see later, neural networks can analyze these images individually (as a radiologist would) or combine them into a single 3D volume to make predictions.\n",
    "\n",
    "At a high level, MRI works by measuring the radio waves emitting by atoms subjected to a magnetic field. \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1740/1*yC1Bt3IOzNv8Pp7t1v7F1Q.png\">\n",
    "\n",
    "In this assignment, we'll build a multi-class segmentation model. We'll  identify 3 different abnormalities in each image: edemas, non-enhancing tumors, and enhancing tumors.\n",
    "\n",
    "<a name=\"1-2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRI Data Processing\n",
    "\n",
    "We often encounter MR images in the [DICOM format](https://en.wikipedia.org/wiki/DICOM). \n",
    "- The DICMO format is the output format for most commercial MRI scanners. This type of data can be processed using the [pydicom](https://pydicom.github.io/pydicom/stable/getting_started.html) Python library. \n",
    "\n",
    "In this assignment, we will be using the data from the [Decathlon 10 Challenge](https://decathlon-10.grand-challenge.org). This data has been mostly pre-processed for the competition participants, however in real practice, MRI data needs to be significantly pre-preprocessed before we can use it to train our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aTg4vp-Eo86-"
   },
   "source": [
    "<a name=\"1-3\"></a>\n",
    "### Exploring the Dataset\n",
    "\n",
    "Our dataset is stored in the [NifTI-1 format](https://nifti.nimh.nih.gov/nifti-1/) and we will be using the [NiBabel library](https://github.com/nipy/nibabel) to interact with the files. Each training sample is composed of two separate files:\n",
    "\n",
    "The first file is an image file containing a 4D array of MR image in the shape of (240, 240, 155, 4). \n",
    "-  The first 3 dimensions are the X, Y, and Z values for each point in the 3D volume, which is commonly called a voxel. \n",
    "- The 4th dimension is the values for 4 different sequences\n",
    "    - 0: FLAIR: \"Fluid Attenuated Inversion Recovery\" (FLAIR)\n",
    "    - 1: T1w: \"T1-weighted\"\n",
    "    - 2: t1gd: \"T1-weighted with gadolinium contrast enhancement\" (T1-Gd)\n",
    "    - 3: T2w: \"T2-weighted\"\n",
    "\n",
    "The second file in each training example is a label file containing a 3D array with the shape of (240, 240, 155).  \n",
    "- The integer values in this array indicate the \"label\" for each voxel in the corresponding image files:\n",
    "    - 0: background\n",
    "    - 1: edema\n",
    "    - 2: non-enhancing tumor\n",
    "    - 3: enhancing tumor\n",
    "\n",
    "We have access to a total of 484 training images which we will be splitting into a training (80%) and validation (20%) dataset.\n",
    "\n",
    "Let's begin by looking at one single case and visualizing the data! You have access to 10 different cases via this notebook and we strongly encourage you to explore the data further on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gqgu96ccW0cJ"
   },
   "source": [
    "We'll use the [NiBabel library](https://nipy.org/nibabel/nibabel_images.html) to load the image and label for a case. The function is shown below to give you a sense of how it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T01:29:36.462907Z",
     "start_time": "2020-03-08T01:29:36.458910Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "AoW-WFWNW0cN"
   },
   "outputs": [],
   "source": [
    "# set home directory and data directory\n",
    "HOME_DIR = r\"E:\\my_work\\2025\\alnafi\\Devops\\python\\mri segmentation\\brain data\\Files\\data\\BraTS-Data\"\n",
    "DATA_DIR = HOME_DIR\n",
    "\n",
    "def load_case(image_nifty_file, label_nifty_file):\n",
    "    # load the image and label file, get the image content and return a numpy array for each\n",
    "    image = np.array(nib.load(image_nifty_file).get_fdata())\n",
    "    label = np.array(nib.load(label_nifty_file).get_fdata())\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CzRraO9TW0cT"
   },
   "source": [
    "We'll now visualize an example.  For this, we use a pre-defined function we have written in the `util.py` file that uses `matplotlib` to generate a summary of the image. \n",
    "\n",
    "The colors correspond to each class.\n",
    "- Red is edema\n",
    "- Green is a non-enhancing tumor\n",
    "- Blue is an enhancing tumor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T01:29:41.377380Z",
     "start_time": "2020-03-08T01:29:38.869873Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "colab_type": "code",
    "id": "ihLR2ZD-W0cU",
    "outputId": "dcf534ac-8c15-46d5-9851-a0fe35e05411"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_categorical' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutil\u001b[39;00m\n\u001b[0;32m      2\u001b[0m image, label \u001b[38;5;241m=\u001b[39m load_case(DATA_DIR \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/imagesTr/BRATS_003.nii.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m, DATA_DIR \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/labelsTr/BRATS_003.nii.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_labeled_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m util\u001b[38;5;241m.\u001b[39mplot_image_grid(image)\n",
      "File \u001b[1;32me:\\my_work\\2025\\alnafi\\Devops\\python\\mri segmentation\\util.py:263\u001b[0m, in \u001b[0;36mget_labeled_image\u001b[1;34m(image, label, is_categorical)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_labeled_image\u001b[39m(image, label, is_categorical\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_categorical:\n\u001b[1;32m--> 263\u001b[0m         label \u001b[38;5;241m=\u001b[39m \u001b[43mto_categorical\u001b[49m(label, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m    265\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mnormalize(image[:, :, :, \u001b[38;5;241m0\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m,\n\u001b[0;32m    266\u001b[0m                           norm_type\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mNORM_MINMAX, dtype\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mCV_32F)\u001b[38;5;241m.\u001b[39mastype(\n\u001b[0;32m    267\u001b[0m         np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m    269\u001b[0m     labeled_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(label[:, :, :, \u001b[38;5;241m1\u001b[39m:])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'to_categorical' is not defined"
     ]
    }
   ],
   "source": [
    "import util\n",
    "image, label = load_case(DATA_DIR + \"/imagesTr/BRATS_003.nii.gz\", DATA_DIR + \"/labelsTr/BRATS_003.nii.gz\")\n",
    "image = util.get_labeled_image(image, label)\n",
    "\n",
    "util.plot_image_grid(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "14GxIOrQW0ce"
   },
   "source": [
    "We've also written a utility function which generates a GIF that shows what it looks like to iterate over each axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T01:29:46.121069Z",
     "start_time": "2020-03-08T01:29:42.475071Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "colab_type": "code",
    "id": "TJubVx44W0cf",
    "outputId": "a5ac10ef-be99-491f-cc04-1ace92434e25"
   },
   "outputs": [],
   "source": [
    "image, label = load_case(DATA_DIR + \"imagesTr/BRATS_002.nii.gz\", DATA_DIR + \"labelsTr/BRATS_002.nii.gz\")\n",
    "util.visualize_data_gif(util.get_labeled_image(image, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dulCuzOnW0ch"
   },
   "source": [
    "<a name=\"1-4\"></a>\n",
    "### Data Preprocessing using Patches\n",
    "\n",
    "While our dataset is provided to us post-registration and in the NIfTI format, we still have to do some minor pre-processing before feeding the data to our model. \n",
    "\n",
    "##### Generate sub-volumes\n",
    "\n",
    "We are going to first generate \"patches\" of our data which you can think of as sub-volumes of the whole MR images. \n",
    "- The reason that we are generating patches is because a network that can process the entire volume at once will simply not fit inside our current environment's memory/GPU.\n",
    "- Therefore we will be using this common technique to generate spatially consistent sub-volumes of our data, which can be fed into our network.\n",
    "- Specifically, we will be generating randomly sampled sub-volumes of shape \\[160, 160, 16\\] from our images. \n",
    "- Furthermore, given that a large portion of the MRI volumes are just brain tissue or black background without any tumors, we want to make sure that we pick patches that at least include some amount of tumor data. \n",
    "- Therefore, we are only going to pick patches that have at most 95% non-tumor regions (so at least 5% tumor). \n",
    "- We do this by filtering the volumes based on the values present in the background labels.\n",
    "\n",
    "##### Standardization (mean 0, stdev 1)\n",
    "\n",
    "Lastly, given that the values in MR images cover a very wide range, we will standardize the values to have a mean of zero and standard deviation of 1. \n",
    "- This is a common technique in deep image processing since standardization makes it much easier for the network to learn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H8GLemPeW0cj"
   },
   "source": [
    "<a name=\"ex-1\"></a>\n",
    "### Get_sub_volume\n",
    "\n",
    "Fill in the function below takes in:\n",
    "- a 4D image (shape: \\[240, 240, 155, 4\\])\n",
    "- its 3D label (shape: \\[240, 240, 155\\]) arrays, \n",
    "\n",
    "The function returns:\n",
    " - A randomly generated sub-volume of size \\[160, 160, 16\\]\n",
    " - Its corresponding label in a 1-hot format which has the shape \\[3, 160, 160, 16\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T01:29:46.222114Z",
     "start_time": "2020-03-08T01:29:46.138071Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Fd3ThBwlPIiH"
   },
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def get_sub_volume(image, label, \n",
    "                   orig_x = 240, orig_y = 240, orig_z = 155, \n",
    "                   output_x = 160, output_y = 160, output_z = 16,\n",
    "                   num_classes = 4, max_tries = 1000, \n",
    "                   background_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Extract random sub-volume from original images.\n",
    "\n",
    "    Args:\n",
    "        image (np.array): original image, \n",
    "            of shape (orig_x, orig_y, orig_z, num_channels)\n",
    "        label (np.array): original label. \n",
    "            labels coded using discrete values rather than\n",
    "            a separate dimension, \n",
    "            so this is of shape (orig_x, orig_y, orig_z)\n",
    "        orig_x (int): x_dim of input image\n",
    "        orig_y (int): y_dim of input image\n",
    "        orig_z (int): z_dim of input image\n",
    "        output_x (int): desired x_dim of output\n",
    "        output_y (int): desired y_dim of output\n",
    "        output_z (int): desired z_dim of output\n",
    "        num_classes (int): number of class labels\n",
    "        max_tries (int): maximum trials to do when sampling\n",
    "        background_threshold (float): limit on the fraction \n",
    "            of the sample which can be the background\n",
    "\n",
    "    returns:\n",
    "        X (np.array): sample of original image of dimension \n",
    "            (num_channels, output_x, output_y, output_z)\n",
    "        y (np.array): labels which correspond to X, of dimension \n",
    "            (num_classes, output_x, output_y, output_z)\n",
    "    \"\"\"\n",
    "    # Initialize features and labels with `None`\n",
    "    X = None\n",
    "    y = None\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    tries = 0\n",
    "    \n",
    "    while tries < max_tries:\n",
    "        # randomly sample sub-volume by sampling the corner voxel\n",
    "        # hint: make sure to leave enough room for the output dimensions!\n",
    "        # do not remove/delete the '0's\n",
    "        start_x = np.random.randint(0, orig_x - output_x + 1)\n",
    "        start_y = np.random.randint(0, orig_y - output_y + 1)\n",
    "        start_z = np.random.randint(0, orig_z - output_z + 1)\n",
    "\n",
    "        # extract relevant area of label\n",
    "        y = label[start_x: start_x + output_x,\n",
    "                  start_y: start_y + output_y,\n",
    "                  start_z: start_z + output_z]\n",
    "        \n",
    "        # One-hot encode the categories.\n",
    "        # This adds a 4th dimension, 'num_classes'\n",
    "        # (output_x, output_y, output_z, num_classes)\n",
    "        y = keras.utils.to_categorical(y, num_classes=num_classes)\n",
    "\n",
    "        # compute the background ratio (this has been implemented for you)\n",
    "        bgrd_ratio = np.sum(y[:, :, :, 0])/(output_x * output_y * output_z)\n",
    "\n",
    "        # increment tries counter\n",
    "        tries += 1\n",
    "\n",
    "        # if background ratio is below the desired threshold,\n",
    "        # use that sub-volume.\n",
    "        # otherwise continue the loop and try another random sub-volume\n",
    "        if bgrd_ratio < background_threshold:\n",
    "\n",
    "            # make copy of the sub-volume\n",
    "            X = np.copy(image[start_x: start_x + output_x,\n",
    "                              start_y: start_y + output_y,\n",
    "                              start_z: start_z + output_z, :])\n",
    "            \n",
    "            # change dimension of X\n",
    "            # from (x_dim, y_dim, z_dim, num_channels)\n",
    "            # to (num_channels, x_dim, y_dim, z_dim)\n",
    "            X = np.transpose(X, (3, 0, 1, 2))\n",
    "\n",
    "            # change dimension of y\n",
    "            # from (x_dim, y_dim, z_dim, num_classes)\n",
    "            # to (num_classes, x_dim, y_dim, z_dim)\n",
    "            y = np.transpose(y, (3, 0, 1, 2))\n",
    "\n",
    "            ### END CODE HERE ###\n",
    "            \n",
    "            # take a subset of y that excludes the background class\n",
    "            # in the 'num_classes' dimension\n",
    "            y = y[1:, :, :, :]\n",
    "    \n",
    "            return X, y\n",
    "\n",
    "    # if we've tried max_tries number of samples\n",
    "    # Give up in order to avoid looping forever.\n",
    "    print(f\"Tried {tries} times to find a sub-volume. Giving up...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T01:29:49.157586Z",
     "start_time": "2020-03-08T01:29:47.726586Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "otA9p2lRW0cu",
    "outputId": "e0888053-f1ca-4afd-e946-2416446d423e"
   },
   "outputs": [],
   "source": [
    "image, label = load_case(DATA_DIR + \"imagesTr/BRATS_001.nii.gz\", DATA_DIR + \"labelsTr/BRATS_001.nii.gz\")\n",
    "X, y = get_sub_volume(image, label)\n",
    "# enhancing tumor is channel 2 in the class label\n",
    "# you can change indexer for y to look at different classes\n",
    "util.visualize_patch(X[0, :, :, :], y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T01:29:49.276586Z",
     "start_time": "2020-03-08T01:29:49.245584Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "u1T5NzNe9Cnp"
   },
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def standardize(image):\n",
    "    \"\"\"\n",
    "    Standardize mean and standard deviation \n",
    "        of each channel and z_dimension.\n",
    "\n",
    "    Args:\n",
    "        image (np.array): input image, \n",
    "            shape (num_channels, dim_x, dim_y, dim_z)\n",
    "\n",
    "    Returns:\n",
    "        standardized_image (np.array): standardized version of input image\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # initialize to array of zeros, with same shape as the image\n",
    "    standardized_image = np.zeros_like(image)\n",
    "\n",
    "    # iterate over channels\n",
    "    for c in range(image.shape[0]):\n",
    "        # iterate over the `z` dimension\n",
    "        for z in range(image.shape[3]):\n",
    "            # get a slice of the image \n",
    "            # at channel c and z-th dimension `z`\n",
    "            image_slice = image[c,:,:,z]\n",
    "\n",
    "            # subtract the mean from image_slice\n",
    "            centered = image_slice - np.mean(image_slice)\n",
    "            \n",
    "            # divide by the standard deviation (only if it is different from zero)\n",
    "            if np.std(centered) != 0:\n",
    "                centered_scaled = centered / np.std(centered)\n",
    "\n",
    "                # update  the slice of standardized image\n",
    "                # with the scaled centered and scaled image\n",
    "            standardized_image[c, :, :, z] = centered_scaled\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return standardized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T01:29:53.531889Z",
     "start_time": "2020-03-08T01:29:53.340902Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "XcMiq-3FOYHe",
    "outputId": "b910aa87-4499-473d-9779-42b30a7a40bc"
   },
   "outputs": [],
   "source": [
    "X_norm = standardize(X)\n",
    "util.visualize_patch(X_norm[0, :, :, :], y[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PnYAP0SQK7NL"
   },
   "source": [
    "<a name=\"2\"></a>\n",
    "## 3D U-Net Model\n",
    "\n",
    "Now let's build our model. In this project we will be building a [3D U-net](https://arxiv.org/abs/1606.06650). \n",
    "- This architecture will take advantage of the volumetric shape of MR images and is one of the best performing models for this task. \n",
    "\n",
    "\n",
    "<img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P0tVtbIshBXq"
   },
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3. Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XOWndz7GecSh"
   },
   "source": [
    "<a name=\"3-1\"></a>\n",
    "### Dice Similarity Coefficient\n",
    "\n",
    "Aside from the architecture, one of the most important elements of any deep learning method is the choice of our loss function. \n",
    "\n",
    "A natural choice that you may be familiar with is the cross-entropy loss function. \n",
    "- However, this loss function is not ideal for segmentation tasks due to heavy class imbalance (there are typically not many positive regions). \n",
    "\n",
    "A much more common loss for segmentation tasks is the Dice similarity coefficient, which is a measure of how well two contours overlap. \n",
    "- The Dice index ranges from 0 (complete mismatch) \n",
    "- To 1 (perfect match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T01:29:58.610988Z",
     "start_time": "2020-03-08T01:29:58.605988Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "09NNAst0BZJE"
   },
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def single_class_dice_coefficient(y_true, y_pred, axis=(0, 1, 2), \n",
    "                                  epsilon=0.00001):\n",
    "    \"\"\"\n",
    "    Compute dice coefficient for single class.\n",
    "\n",
    "    Args:\n",
    "        y_true (Tensorflow tensor): tensor of ground truth values for single class.\n",
    "                                    shape: (x_dim, y_dim, z_dim)\n",
    "        y_pred (Tensorflow tensor): tensor of predictions for single class.\n",
    "                                    shape: (x_dim, y_dim, z_dim)\n",
    "        axis (tuple): spatial axes to sum over when computing numerator and\n",
    "                      denominator of dice coefficient.\n",
    "                      Hint: pass this as the 'axis' argument to the K.sum function.\n",
    "        epsilon (float): small constant added to numerator and denominator to\n",
    "                        avoid divide by 0 errors.\n",
    "    Returns:\n",
    "        dice_coefficient (float): computed value of dice coefficient.     \n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    dice_numerator = 2*(K.sum(y_true*y_pred, axis= axis)) + epsilon\n",
    "    dice_denominator = K.sum(y_true, axis = axis) + K.sum(y_pred, axis = axis) + epsilon\n",
    "    dice_coefficient = dice_numerator/dice_denominator\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return dice_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T01:30:03.312564Z",
     "start_time": "2020-03-08T01:30:03.307566Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "AAmG3BP0EJFk"
   },
   "outputs": [],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def dice_coefficient(y_true, y_pred, axis=(1, 2, 3), \n",
    "                     epsilon=0.00001):\n",
    "    \"\"\"\n",
    "    Compute mean dice coefficient over all abnormality classes.\n",
    "\n",
    "    Args:\n",
    "        y_true (Tensorflow tensor): tensor of ground truth values for all classes.\n",
    "                                    shape: (num_classes, x_dim, y_dim, z_dim)\n",
    "        y_pred (Tensorflow tensor): tensor of predictions for all classes.\n",
    "                                    shape: (num_classes, x_dim, y_dim, z_dim)\n",
    "        axis (tuple): spatial axes to sum over when computing numerator and\n",
    "                      denominator of dice coefficient.\n",
    "                      Hint: pass this as the 'axis' argument to the K.sum function.\n",
    "        epsilon (float): small constant add to numerator and denominator to\n",
    "                        avoid divide by 0 errors.\n",
    "    Returns:\n",
    "        dice_coefficient (float): computed value of dice coefficient.     \n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    dice_numerator = 2*(K.sum(y_true*y_pred, axis= axis)) + epsilon\n",
    "    dice_denominator = K.sum(y_true, axis = axis) + K.sum(y_pred, axis = axis) + epsilon\n",
    "    dice_coefficient = K.mean(dice_numerator/dice_denominator)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return dice_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T01:30:07.734304Z",
     "start_time": "2020-03-08T01:30:07.728305Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "KjtevmR185vb"
   },
   "outputs": [],
   "source": [
    "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def soft_dice_loss(y_true, y_pred, axis=(1, 2, 3), \n",
    "                   epsilon=0.00001):\n",
    "    \"\"\"\n",
    "    Compute mean soft dice loss over all abnormality classes.\n",
    "\n",
    "    Args:\n",
    "        y_true (Tensorflow tensor): tensor of ground truth values for all classes.\n",
    "                                    shape: (num_classes, x_dim, y_dim, z_dim)\n",
    "        y_pred (Tensorflow tensor): tensor of soft predictions for all classes.\n",
    "                                    shape: (num_classes, x_dim, y_dim, z_dim)\n",
    "        axis (tuple): spatial axes to sum over when computing numerator and\n",
    "                      denominator in formula for dice loss.\n",
    "                      Hint: pass this as the 'axis' argument to the K.sum function.\n",
    "        epsilon (float): small constant added to numerator and denominator to\n",
    "                        avoid divide by 0 errors.\n",
    "    Returns:\n",
    "        dice_loss (float): computed value of dice loss.     \n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "\n",
    "    dice_numerator = 2*(K.sum(y_true*y_pred, axis= axis)) + epsilon\n",
    "    dice_denominator = K.sum(y_true**2, axis = axis) + K.sum(y_pred**2, axis = axis) + epsilon\n",
    "    dice_loss =1-K.mean(dice_numerator/dice_denominator)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return dice_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4. Create and Train the Model\n",
    "\n",
    "Once we've finished implementing the soft dice loss, we can create the model! \n",
    "\n",
    "We'll use the `unet_model_3d` function in `utils` which we implemented for you.\n",
    "- This creates the model architecture and compiles the model with the specified loss functions and metrics. \n",
    "- Check out function `util.unet_model_3d(loss_function)` in the `util.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = util.unet_model_3d(loss_function=soft_dice_loss, metrics=[dice_coefficient])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bcBeF80jf54b"
   },
   "source": [
    "```Python\n",
    "# Run this on your local machine only\n",
    "base_dir = HOME_DIR + \"processed/\"\n",
    "\n",
    "with open(base_dir + \"config.json\") as json_file:\n",
    "    config = json.load(json_file)\n",
    "\n",
    "# Get generators for training and validation sets\n",
    "train_generator = util.VolumeDataGenerator(config[\"train\"], base_dir + \"train/\", batch_size=3, dim=(160, 160, 16), verbose=0)\n",
    "valid_generator = util.VolumeDataGenerator(config[\"valid\"], base_dir + \"valid/\", batch_size=3, dim=(160, 160, 16), verbose=0)\n",
    "\n",
    "steps_per_epoch = 20\n",
    "n_epochs=10\n",
    "validation_steps = 20\n",
    "\n",
    "model.fit_generator(generator=train_generator,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=n_epochs,\n",
    "        use_multiprocessing=True,\n",
    "        validation_data=valid_generator,\n",
    "        validation_steps=validation_steps)\n",
    "\n",
    "#model.save_weights(base_dir + 'my_model_pretrained.hdf5')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Oq1qG5UW0dY"
   },
   "source": [
    "<a name=\"4-2\"></a>\n",
    "### Loading a Pre-Trained Model\n",
    "\n",
    "Instead of having the model train for longer, we'll give you access to a pretrained version. We'll use this to extract predictions and measure performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell if you didn't run the training cell in section 4.1\n",
    "base_dir = HOME_DIR + \"processed/\"\n",
    "with open(base_dir + \"config.json\") as json_file:\n",
    "    config = json.load(json_file)\n",
    "# Get generators for training and validation sets\n",
    "train_generator = util.VolumeDataGenerator(config[\"train\"], base_dir + \"train/\", batch_size=3, dim=(160, 160, 16), verbose=0)\n",
    "valid_generator = util.VolumeDataGenerator(config[\"valid\"], base_dir + \"valid/\", batch_size=3, dim=(160, 160, 16), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iYJ3cdSGeR5l"
   },
   "outputs": [],
   "source": [
    "model.load_weights(HOME_DIR + \"model_pretrained.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "22JSeC5yOnty"
   },
   "source": [
    "<a name=\"5\"></a>\n",
    "## Evaluation\n",
    "\n",
    "Now that we have a trained model, we'll learn to extract its predictions and evaluate its performance on scans from our validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DjK9oMJ3iEeW"
   },
   "source": [
    "<a name=\"5-1\"></a>\n",
    "### Overall Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "akLh2sTIhkTj"
   },
   "source": [
    "First let's measure the overall performance on the validation set. \n",
    "- We can do this by calling the keras [evaluate_generator](https://keras.io/models/model/#evaluate_generator) function and passing in the validation generator, created in section 4.1. \n",
    "\n",
    "#### Using the validation set for testing\n",
    "- Note: since we didn't do cross validation tuning on the final model, it's okay to use the validation set.\n",
    "- For real life implementations, however, you would want to do cross validation as usual to choose hyperparamters and then use a hold out test set to assess performance\n",
    "\n",
    "Python Code for measuring the overall performance on the validation set:\n",
    "\n",
    "```python\n",
    "val_loss, val_dice = model.evaluate_generator(valid_generator)\n",
    "\n",
    "print(f\"validation soft dice loss: {val_loss:.4f}\")\n",
    "print(f\"validation dice coefficient: {val_dice:.4f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JGZ-GLXPiCXH"
   },
   "source": [
    "<a name=\"5-2\"></a>\n",
    "### 5.2 Patch-level Predictions\n",
    "\n",
    "When applying the model, we'll want to look at segmentations for individual scans (entire scans, not just the sub-volumes)\n",
    "- This will be a bit complicated because of our sub-volume approach. \n",
    "- First let's keep things simple and extract model predictions for sub-volumes.\n",
    "- We can use the sub-volume which we extracted at the beginning of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "D3Zx9gSiAhEC",
    "outputId": "4fb5d166-2dbe-4cfc-84ce-8ebab9867729"
   },
   "outputs": [],
   "source": [
    "util.visualize_patch(X_norm[0, :, :, :], y[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BRJF6bR9i4n7"
   },
   "source": [
    "#### Add a 'batch' dimension\n",
    "We can extract predictions by calling `model.predict` on the patch. \n",
    "- We'll add an `images_per_batch` dimension, since the `predict` method is written to take in batches. \n",
    "- The dimensions of the input should be `(images_per_batch, num_channels, x_dim, y_dim, z_dim)`.\n",
    "- Use [numpy.expand_dims](https://docs.scipy.org/doc/numpy/reference/generated/numpy.expand_dims.html) to add a new dimension as the zero-th dimension by setting axis=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_GKVqDNbjUIF"
   },
   "outputs": [],
   "source": [
    "X_norm_with_batch_dimension = np.expand_dims(X_norm, axis=0)\n",
    "patch_pred = model.predict(X_norm_with_batch_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c11FN5SJjXxT"
   },
   "source": [
    "#### Convert prediction from probability into a category\n",
    "Currently, each element of `patch_pred` is a number between 0.0 and 1.0.\n",
    "- Each number is the model's confidence that a voxel is part of a given class. \n",
    "- You will convert these to discrete 0 and 1 integers by using a threshold. \n",
    "- We'll use a threshold of 0.5. \n",
    "- In real applications, you would tune this to achieve your required level  of sensitivity or specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VCsVNiKJBvcC"
   },
   "outputs": [],
   "source": [
    "# set threshold.\n",
    "threshold = 0.5\n",
    "\n",
    "# use threshold to get hard predictions\n",
    "patch_pred[patch_pred > threshold] = 1.0\n",
    "patch_pred[patch_pred <= threshold] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AenKy0OGjs-C"
   },
   "source": [
    "Now let's visualize the original patch and ground truth alongside our thresholded predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "colab_type": "code",
    "id": "vf6N-lzLjov4",
    "outputId": "071f06c1-a440-4267-99bf-cf261c82efff"
   },
   "outputs": [],
   "source": [
    "print(\"Patch and ground truth\")\n",
    "util.visualize_patch(X_norm[0, :, :, :], y[2])\n",
    "plt.show()\n",
    "print(\"Patch and prediction\")\n",
    "util.visualize_patch(X_norm[0, :, :, :], patch_pred[0, 2, :, :, :])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_JAEdpA3llCJ"
   },
   "source": [
    "<a name=\"5-2-1\"></a>\n",
    "#### Sensitivity and Specificity\n",
    "\n",
    " \n",
    "- To quantify its performance, we can use per-pixel sensitivity and specificity. \n",
    "\n",
    "Recall that in terms of the true positives, true negatives, false positives, and false negatives, \n",
    "\n",
    "$$\\text{sensitivity} = \\frac{\\text{true positives}}{\\text{true positives} + \\text{false negatives}}$$\n",
    "\n",
    "$$\\text{specificity} = \\frac{\\text{true negatives}}{\\text{true negatives} + \\text{false positives}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qnc9IWfcX1YJ"
   },
   "outputs": [],
   "source": [
    "# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def compute_class_sens_spec(pred, label, class_num):\n",
    "    \"\"\"\n",
    "    Compute sensitivity and specificity for a particular example\n",
    "    for a given class.\n",
    "\n",
    "    Args:\n",
    "        pred (np.array): binary arrary of predictions, shape is\n",
    "                         (num classes, height, width, depth).\n",
    "        label (np.array): binary array of labels, shape is\n",
    "                          (num classes, height, width, depth).\n",
    "        class_num (int): number between 0 - (num_classes -1) which says\n",
    "                         which prediction class to compute statistics\n",
    "                         for.\n",
    "\n",
    "    Returns:\n",
    "        sensitivity (float): for a given class_num.\n",
    "        specificity (float): for a given class_num.\n",
    "    \"\"\"\n",
    "\n",
    "    # extract sub-array for specified class\n",
    "    class_pred = pred[class_num]\n",
    "    class_label = label[class_num]\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # compute:\n",
    "    \n",
    "    # true positives\n",
    "    tp = np.sum(np.logical_and(class_pred == 1, class_label == 1))\n",
    "\n",
    "    # true negatives\n",
    "    tn = np.sum(np.logical_and(class_pred == 0, class_label == 0))\n",
    "    \n",
    "    #false positives\n",
    "    fp = np.sum(np.logical_and(class_pred == 1, class_label == 0))\n",
    "    \n",
    "    # false negatives\n",
    "    fn = np.sum(np.logical_and(class_pred == 0, class_label == 1))\n",
    "\n",
    "    # compute sensitivity and specificity\n",
    "    sensitivity = tp / (tp + fn) \n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return sensitivity, specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensitivity and Specificity for the patch prediction\n",
    "\n",
    "Next let's compute the sensitivity and specificity on that patch for expanding tumors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Kp4SJDWmc0L5",
    "outputId": "f8735867-131d-4d25-d84d-e008e9a68d5f"
   },
   "outputs": [],
   "source": [
    "sensitivity, specificity = compute_class_sens_spec(patch_pred[0], y, 2)\n",
    "\n",
    "print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MdSeKsZEntHG"
   },
   "source": [
    "We can also display the sensitivity and specificity for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0LuVZHUKnp4t"
   },
   "outputs": [],
   "source": [
    "def get_sens_spec_df(pred, label):\n",
    "    patch_metrics = pd.DataFrame(\n",
    "        columns = ['Edema', \n",
    "                   'Non-Enhancing Tumor', \n",
    "                   'Enhancing Tumor'], \n",
    "        index = ['Sensitivity',\n",
    "                 'Specificity'])\n",
    "    \n",
    "    for i, class_name in enumerate(patch_metrics.columns):\n",
    "        sens, spec = compute_class_sens_spec(pred, label, i)\n",
    "        patch_metrics.loc['Sensitivity', class_name] = round(sens,4)\n",
    "        patch_metrics.loc['Specificity', class_name] = round(spec,4)\n",
    "\n",
    "    return patch_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "lBPqWFmspQHj",
    "outputId": "7efafc97-ed71-4e28-ae74-4be3d5533322"
   },
   "outputs": [],
   "source": [
    "df = get_sens_spec_df(patch_pred[0], y)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "colab_type": "code",
    "id": "-DUANuJD2_sm",
    "outputId": "d008adb5-f69c-4b98-e886-e70e8d759ed7"
   },
   "outputs": [],
   "source": [
    " #uncomment this code to run it\n",
    "image, label = load_case(DATA_DIR + \"imagesTr/BRATS_001.nii.gz\", DATA_DIR + \"labelsTr/BRATS_001.nii.gz\")\n",
    "pred = util.predict_and_viz(image, label, model, .5, loc=(130, 130, 77))                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "colab_type": "code",
    "id": "18m8pkA9W0dt",
    "outputId": "673151b3-2120-4609-f0d6-154b47e8516a"
   },
   "outputs": [],
   "source": [
    "image, label = load_case(DATA_DIR + \"imagesTr/BRATS_003.nii.gz\", DATA_DIR + \"labelsTr/BRATS_003.nii.gz\")\n",
    "pred = util.predict_and_viz(image, label, model, .5, loc=(130, 130, 77))                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T01:50:20.141287Z",
     "start_time": "2020-03-08T01:50:20.138316Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "H7pB-ZgPsl2N"
   },
   "outputs": [],
   "source": [
    "whole_scan_label = keras.utils.to_categorical(label, num_classes = 4)\n",
    "whole_scan_pred = pred\n",
    "\n",
    "# move axis to match shape expected in functions\n",
    "whole_scan_label = np.moveaxis(whole_scan_label, 3 ,0)[1:4]\n",
    "whole_scan_pred = np.moveaxis(whole_scan_pred, 3, 0)[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "tpljPNBJ6k0k",
    "outputId": "adb78f68-6de9-4c8e-9ffb-163458a68b2f"
   },
   "outputs": [],
   "source": [
    "whole_scan_df = get_sens_spec_df(whole_scan_pred, whole_scan_label)\n",
    "\n",
    "print(whole_scan_df)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "C1A4_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf210_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
